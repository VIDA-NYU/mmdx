import os
import lancedb
import pandas as pd
import pyarrow as pa
from typing import Iterator, List, Optional
import random
import tqdm
from .model import BaseEmbeddingModel
from .settings import (
    DB_BATCH_SIZE,
    DATA_SAMPLE_SIZE,
    IMAGE_EXTENSIONS,
    CSV_FILENAME,
    DEFAULT_CSV_BUCKET,
)
import imghdr
from io import BytesIO
from .s3_client import S3Client


def detect_image_type(image_path: str) -> Optional[str]:
    return imghdr.what(image_path)


def find_files_in_path(path, file_extensions=IMAGE_EXTENSIONS) -> Iterator[str]:
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            relative_dir = dirpath.replace(path, "")
            file_path = os.path.join(relative_dir, filename)
            if filename.lower().endswith(file_extensions):
                yield file_path
            else:
                absolute_path = os.path.join(dirpath, filename)
                image_type = detect_image_type(absolute_path)
                if f".{image_type}" in file_extensions:
                    yield file_path


def load_images_from_path(data_path: str, sample_size=DATA_SAMPLE_SIZE):
    image_files = list(find_files_in_path(data_path))
    print(f"Found {len(image_files)} images in {data_path}")
    if sample_size is not None:
        print(f"Sampling {sample_size} out of {len(image_files)} images...")
        image_files = random.sample(image_files, sample_size)
    return image_files


def load_images_from_minio(
    data_path: str, S3_Client: S3Client, sample_size=DATA_SAMPLE_SIZE
):
    image_files, df = get_image_files(data_path, S3_Client)
    print(f"Found {len(image_files)} images in {data_path}")
    if sample_size is not None:
        print(f"Sampling {sample_size} out of {len(image_files)} images...")
        image_files = random.sample(image_files, sample_size)
    return image_files, df


def get_image_files(data_path: str, S3_Client: S3Client):
    if CSV_FILENAME:
        print("Getting images from CSV file")
        csv_data = S3_Client.get_obj(DEFAULT_CSV_BUCKET, CSV_FILENAME)
        df = pd.read_csv(BytesIO(csv_data.read()))
        image_files = df["image_path"].to_list()
    elif os.environ.get("CSV_PATH"):
        csv_path = os.environ.get("CSV_PATH")
        print("Getting images from CSV upload by user")
        df = pd.read_csv(csv_path)
        image_files = df["image_path"].to_list()
    else:
        image_files = S3_Client.list_objects_names(data_path)
        df = None

    return image_files, df


def embed_image_files(
    model: BaseEmbeddingModel,
    data_path: str,
    image_paths: List[str],
    S3_Client: Optional[S3Client],
):
    embeddings = []
    for path in image_paths:
        try:
            if isinstance(S3_Client, S3Client):
                image_data = S3_Client.get_obj(data_path, path)
                image_buffer = BytesIO(image_data.read())
                embedding = model.embed_image_path(image_buffer)
            else:
                embedding = model.embed_image_path(
                    image_path=os.path.join(data_path, path)
                )
            embeddings.append((path, embedding))
        except Exception as e:
            print(f"Failed to create embbeding for image {path}.", e)
            embeddings.append((path, None))
    return embeddings


def load_batches(
    db: lancedb.DBConnection,
    table_name: str,
    data_path: str,
    S3_Client: Optional[S3Client],
    model: BaseEmbeddingModel,
    batch_size=DB_BATCH_SIZE,
) -> lancedb.table.Table:
    if isinstance(S3_Client, S3Client):
        image_files, df = load_images_from_minio(data_path, S3_Client)
    else:
        image_files = load_images_from_path(data_path)

    def make_batches() -> Iterator[pa.RecordBatch]:
        with tqdm.tqdm(total=len(image_files)) as progress_bar:
            for i in range(0, len(image_files), batch_size):
                image_paths = image_files[i : i + batch_size]

                results = embed_image_files(model, data_path, image_paths, S3_Client)
                valid_image_paths = [path for path, emb in results if emb is not None]
                valid_embeddings = [emb for path, emb in results if emb is not None]
                valid_title = (
                    df[df["image_path"].isin(valid_image_paths)]["title"]
                    .str.encode("utf-8", errors="replace")
                    .str.decode("utf-8")
                    .tolist()
                )

                image_paths_array = pa.array(valid_image_paths)
                embeddings_array = pa.array(
                    valid_embeddings, pa.list_(pa.float32(), model.dimensions())
                )
                titles_array = pa.array(valid_title)

                progress_bar.update(len(image_paths))

                yield pa.RecordBatch.from_arrays(
                    [
                        titles_array,
                        image_paths_array,
                        embeddings_array,
                    ],
                    [
                        "title",
                        "image_path",
                        "vector",
                    ],
                )

    db_schema = pa.schema(
        [
            pa.field("title", pa.utf8()),
            pa.field("image_path", pa.utf8()),
            pa.field("vector", pa.list_(pa.float32(), model.dimensions())),
        ]
    )
    tbl = db.create_table(
        table_name,
        make_batches(),
        schema=db_schema,
    )
    return tbl


def make_df(
    data_path: str, model: BaseEmbeddingModel, S3_Client: Optional[S3Client]
):
    if S3_Client:
        image_files, df = load_images_from_minio(data_path, S3_Client)
    else:
        image_files = load_images_from_path(data_path)

    titles = []
    image_paths = []
    vectors = []
    metadatas = []
    for image_path in tqdm.tqdm(image_files):
        image_path, embedding = embed_image_files(
            model, data_path, [image_path], S3_Client
        )[0]
        if embedding is not None:
            vectors.append(embedding)
            image_paths.append(image_path)
            titles.append(df.loc[df["image_path"] == image_path, "title"].values[0])
            metadatas.append(df.loc[df["image_path"] == image_path, "metadata"].values[0])

    df = pd.DataFrame(
        {
            "title": titles,
            "metadata": metadatas,
            "image_path": image_paths,
            "vector": vectors,
        }
    )
    return df


def load_df(
    db: lancedb.DBConnection,
    table_name: str,
    data_path: str,
    model: BaseEmbeddingModel,
    S3_Client: Optional[S3Client],
) -> lancedb.table.Table:
    return db.create_table(table_name, data=make_df(data_path, model, S3_Client))
